{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28303c06",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7098b4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "855b364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.name == 'posix':\n",
    "    plt.rc(\"font\", family=\"AppleGothic\")\n",
    "else:\n",
    "    plt.rc(\"font\", family=\"Malgun Gothic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99df1325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플 개수:  506\n"
     ]
    }
   ],
   "source": [
    "dataset = load_boston()\n",
    "\n",
    "X = dataset.data\n",
    "y = dataset.target[:, np.newaxis]  # np.newaxis: (506,) -> (506, 1)\n",
    "\n",
    "print('샘플 개수: ', X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22fa96c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((506, 13), (506, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5e8b848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24.  21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 15.  18.9 21.7 20.4\n",
      " 18.2 19.9 23.1 17.5 20.2 18.2 13.6 19.6 15.2 14.5 15.6 13.9 16.6 14.8\n",
      " 18.4 21.  12.7 14.5 13.2 13.1 13.5 18.9 20.  21.  24.7 30.8 34.9 26.6\n",
      " 25.3 24.7 21.2 19.3 20.  16.6 14.4 19.4 19.7 20.5 25.  23.4 18.9 35.4\n",
      " 24.7 31.6 23.3 19.6 18.7 16.  22.2 25.  33.  23.5 19.4 22.  17.4 20.9\n",
      " 24.2 21.7 22.8 23.4 24.1 21.4 20.  20.8 21.2 20.3 28.  23.9 24.8 22.9\n",
      " 23.9 26.6 22.5 22.2 23.6 28.7 22.6 22.  22.9 25.  20.6 28.4 21.4 38.7\n",
      " 43.8 33.2 27.5 26.5 18.6 19.3 20.1 19.5 19.5 20.4 19.8 19.4 21.7 22.8\n",
      " 18.8 18.7 18.5 18.3 21.2 19.2 20.4 19.3 22.  20.3 20.5 17.3 18.8 21.4\n",
      " 15.7 16.2 18.  14.3 19.2 19.6 23.  18.4 15.6 18.1 17.4 17.1 13.3 17.8\n",
      " 14.  14.4 13.4 15.6 11.8 13.8 15.6 14.6 17.8 15.4 21.5 19.6 15.3 19.4\n",
      " 17.  15.6 13.1 41.3 24.3 23.3 27.  50.  50.  50.  22.7 25.  50.  23.8\n",
      " 23.8 22.3 17.4 19.1 23.1 23.6 22.6 29.4 23.2 24.6 29.9 37.2 39.8 36.2\n",
      " 37.9 32.5 26.4 29.6 50.  32.  29.8 34.9 37.  30.5 36.4 31.1 29.1 50.\n",
      " 33.3 30.3 34.6 34.9 32.9 24.1 42.3 48.5 50.  22.6 24.4 22.5 24.4 20.\n",
      " 21.7 19.3 22.4 28.1 23.7 25.  23.3 28.7 21.5 23.  26.7 21.7 27.5 30.1\n",
      " 44.8 50.  37.6 31.6 46.7 31.5 24.3 31.7 41.7 48.3 29.  24.  25.1 31.5\n",
      " 23.7 23.3 22.  20.1 22.2 23.7 17.6 18.5 24.3 20.5 24.5 26.2 24.4 24.8\n",
      " 29.6 42.8 21.9 20.9 44.  50.  36.  30.1 33.8 43.1 48.8 31.  36.5 22.8\n",
      " 30.7 50.  43.5 20.7 21.1 25.2 24.4 35.2 32.4 32.  33.2 33.1 29.1 35.1\n",
      " 45.4 35.4 46.  50.  32.2 22.  20.1 23.2 22.3 24.8 28.5 37.3 27.9 23.9\n",
      " 21.7 28.6 27.1 20.3 22.5 29.  24.8 22.  26.4 33.1 36.1 28.4 33.4 28.2\n",
      " 22.8 20.3 16.1 22.1 19.4 21.6 23.8 16.2 17.8 19.8 23.1 21.  23.8 23.1\n",
      " 20.4 18.5 25.  24.6 23.  22.2 19.3 22.6 19.8 17.1 19.4 22.2 20.7 21.1\n",
      " 19.5 18.5 20.6 19.  18.7 32.7 16.5 23.9 31.2 17.5 17.2 23.1 24.5 26.6\n",
      " 22.9 24.1 18.6 30.1 18.2 20.6 17.8 21.7 22.7 22.6 25.  19.9 20.8 16.8\n",
      " 21.9 27.5 21.9 23.1 50.  50.  50.  50.  50.  13.8 13.8 15.  13.9 13.3\n",
      " 13.1 10.2 10.4 10.9 11.3 12.3  8.8  7.2 10.5  7.4 10.2 11.5 15.1 23.2\n",
      "  9.7 13.8 12.7 13.1 12.5  8.5  5.   6.3  5.6  7.2 12.1  8.3  8.5  5.\n",
      " 11.9 27.9 17.2 27.5 15.  17.2 17.9 16.3  7.   7.2  7.5 10.4  8.8  8.4\n",
      " 16.7 14.2 20.8 13.4 11.7  8.3 10.2 10.9 11.   9.5 14.5 14.1 16.1 14.3\n",
      " 11.7 13.4  9.6  8.7  8.4 12.8 10.5 17.1 18.4 15.4 10.8 11.8 14.9 12.6\n",
      " 14.1 13.  13.4 15.2 16.1 17.8 14.9 14.1 12.7 13.5 14.9 20.  16.4 17.7\n",
      " 19.5 20.2 21.4 19.9 19.  19.1 19.1 20.1 19.9 19.6 23.2 29.8 13.8 13.3\n",
      " 16.7 12.  14.6 21.4 23.  23.7 25.  21.8 20.6 21.2 19.1 20.6 15.2  7.\n",
      "  8.1 13.6 20.1 21.8 24.5 23.1 19.7 18.3 21.2 17.5 16.8 22.4 20.6 23.9\n",
      " 22.  11.9]\n"
     ]
    }
   ],
   "source": [
    "print(dataset.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "949d57ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.3200e-03 1.8000e+01 2.3100e+00 ... 1.5300e+01 3.9690e+02 4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9690e+02 9.1400e+00]\n",
      " [2.7290e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9283e+02 4.0300e+00]\n",
      " ...\n",
      " [6.0760e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 5.6400e+00]\n",
      " [1.0959e-01 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9345e+02 6.4800e+00]\n",
      " [4.7410e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 7.8800e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c8d770",
   "metadata": {},
   "source": [
    "h: hypothesis를 의미. 즉, $h(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n$\n",
    "<br>\n",
    "cost: $\\frac{1}{2N}\\Sigma^{N}_{i=1}(h(x_{i})-y_{i})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a18a1fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression 비용함수 (최소제곱법)\n",
    "def compute_cost(X, y, params):\n",
    "    n_samples = len(y)\n",
    "    h = X @ params  # @: 내적\n",
    "    return (1/(2*n_samples)) * np.sum((h-y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ee07a1",
   "metadata": {},
   "source": [
    "Gradient update\n",
    "<br>\n",
    "$\\theta = \\theta - \\alpha \\frac{\\partial J(x)}{\\partial \\theta}$\n",
    "<br>\n",
    "$J(x)= \\frac{1}{2N}\\Sigma^{N}_{i=1}(h(x_{i})-y_{i})^2$\n",
    "<br>\n",
    "$J(x)= \\frac{1}{2N}[(h(x_1)-y_1)^2 + (h(x_2)-y_2)^2 + (h(x_3)-y_3)^2 + ... + (h(x_n)-y_n)^2)]$\n",
    "<br>\n",
    "$\\frac{\\partial J(x)}{\\partial \\theta_i} = 2\\frac{1}{2N}(h(x_i)-y_i)(x_i) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d53ffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사하강법 -> Loss가 가장 적은 점까지 params 업데이트\n",
    "def gradient_descent(X, y, params, learning_rate, n_iters):\n",
    "    n_samples = len(y)\n",
    "    J_history = np.zeros((n_iters, 1))\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        params = params - (learning_rate/n_samples) * X.T @ (X @ params - y)\n",
    "        J_history[i] = compute_cost(X, y, params)  # Loss값 기록\n",
    "    \n",
    "    return (J_history, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f6ee4f",
   "metadata": {},
   "source": [
    "### Normalization vs Standardization vs Regularization\n",
    "\n",
    "#### Normalization\n",
    "- 값의 범위(scale)를 0 ~ 1 사이의 값을 바꾸는 것이다.\n",
    "- 학습 전에 scaling한다.\n",
    "    - 머신러닝에서 scale이 큰 feature의 영향이 비대해지는 것을 방지한다.\n",
    "    - 딥러닝에서 Local Minima에 빠질 위험을 감소시킨다.(학습 속도 향상)\n",
    "- Scikit-learn에서 MinMaxScaler\n",
    "$$ \\frac{x - x_{min}}{x_{max} - x_{min}} $$\n",
    "\n",
    "#### Standardization\n",
    "- 값의 범위(scale)를 평균이 0, 분산이 1이 되도록 변환한다.\n",
    "- 학습 전에 scaling한다.\n",
    "    - 머신러닝에서 scale이 큰 feature의 영향이 비대해지는 것을 방지한다.\n",
    "    - 딥러닝에서 Local Minima에 빠질 위험을 감소시킨다.(학습 속도 향상)\n",
    "- 정규분포를 표준정규분포로 변환하는 것과 같다.\n",
    "    - Z-score(표준 점수)\n",
    "    - -1 ~ 1 사이에 68%, -2 ~ 2 사이에 95%, -3 ~ 3 사이에 99%\n",
    "    - -3 ~ 3의 범위를 벗어나면 outlier일 확률이 높다.\n",
    "- 표준화로 번역하기도 한다.\n",
    "- Scikit-learn에서 StandardScaler\n",
    "$$ \\frac{x - \\mu}{\\sigma} $$\n",
    "\n",
    "#### Regularization\n",
    "- weight를 조정하는데 규제를 거는 기법이다.\n",
    "- Overfitting을 막기 위해 사용한다.\n",
    "- L1 regularization, L2 regularization 등\n",
    "    - L1: LASSO(라쏘), 마름모\n",
    "    - L2: RIDGE(릿지), 원\n",
    "$$ Loss = \\frac{1}{n}\\sum_{i=1}^n \\{(y-\\hat{y})^2 + \\frac{\\lambda}{2}|w|^2 \\} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ed3f80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "개수:  506\n",
      "\n",
      "각 컬럼의 평균: \n",
      " [3.61352356e+00 1.13636364e+01 1.11367787e+01 6.91699605e-02\n",
      " 5.54695059e-01 6.28463439e+00 6.85749012e+01 3.79504269e+00\n",
      " 9.54940711e+00 4.08237154e+02 1.84555336e+01 3.56674032e+02\n",
      " 1.26530632e+01]\n",
      "\n",
      "각 컬럼의 표준편차: \n",
      " [8.59304135e+00 2.32993957e+01 6.85357058e+00 2.53742935e-01\n",
      " 1.15763115e-01 7.01922514e-01 2.81210326e+01 2.10362836e+00\n",
      " 8.69865112e+00 1.68370495e+02 2.16280519e+00 9.12046075e+01\n",
      " 7.13400164e+00]\n"
     ]
    }
   ],
   "source": [
    "# standardization 요소 준비\n",
    "n_samples = len(y)\n",
    "mu = np.mean(X, 0)\n",
    "sigma = np.std(X, 0)\n",
    "\n",
    "print('개수: ', n_samples)\n",
    "print()\n",
    "print('각 컬럼의 평균: \\n', mu)\n",
    "print()\n",
    "print('각 컬럼의 표준편차: \\n', sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee6f46ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardization 이전: \n",
      "[[6.3200e-03 1.8000e+01 2.3100e+00 ... 1.5300e+01 3.9690e+02 4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9690e+02 9.1400e+00]\n",
      " [2.7290e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9283e+02 4.0300e+00]\n",
      " ...\n",
      " [6.0760e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 5.6400e+00]\n",
      " [1.0959e-01 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9345e+02 6.4800e+00]\n",
      " [4.7410e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 7.8800e+00]]\n",
      "Standardization 이후: \n",
      "[[-0.41978194  0.28482986 -1.2879095  ... -1.45900038  0.44105193\n",
      "  -1.0755623 ]\n",
      " [-0.41733926 -0.48772236 -0.59338101 ... -0.30309415  0.44105193\n",
      "  -0.49243937]\n",
      " [-0.41734159 -0.48772236 -0.59338101 ... -0.30309415  0.39642699\n",
      "  -1.2087274 ]\n",
      " ...\n",
      " [-0.41344658 -0.48772236  0.11573841 ...  1.17646583  0.44105193\n",
      "  -0.98304761]\n",
      " [-0.40776407 -0.48772236  0.11573841 ...  1.17646583  0.4032249\n",
      "  -0.86530163]\n",
      " [-0.41500016 -0.48772236  0.11573841 ...  1.17646583  0.44105193\n",
      "  -0.66905833]]\n"
     ]
    }
   ],
   "source": [
    "print('Standardization 이전: ')\n",
    "print(X)\n",
    "X = (X-mu) / sigma\n",
    "print('Standardization 이후: ')  # 평균: 0, 분산: 1\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b77b8b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((n_samples, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1595db36",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((np.ones((n_samples, 1)), X))  # bias를 추가해줍니다: y = ax + b -> b\n",
    "n_features = np.size(X, 1)\n",
    "params = np.zeros((n_features, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57dd5ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.41978194,  0.28482986, ..., -1.45900038,\n",
       "         0.44105193, -1.0755623 ],\n",
       "       [ 1.        , -0.41733926, -0.48772236, ..., -0.30309415,\n",
       "         0.44105193, -0.49243937],\n",
       "       [ 1.        , -0.41734159, -0.48772236, ..., -0.30309415,\n",
       "         0.39642699, -1.2087274 ],\n",
       "       ...,\n",
       "       [ 1.        , -0.41344658, -0.48772236, ...,  1.17646583,\n",
       "         0.44105193, -0.98304761],\n",
       "       [ 1.        , -0.40776407, -0.48772236, ...,  1.17646583,\n",
       "         0.4032249 , -0.86530163],\n",
       "       [ 1.        , -0.41500016, -0.48772236, ...,  1.17646583,\n",
       "         0.44105193, -0.66905833]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a1e28d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3410eb04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a619414e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 14)\n",
      "(14, 1)\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(params.shape)\n",
    "print(n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f912ce67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 Loss:  296.0734584980237 \n",
      "\n",
      "업데이트 된 파라미터: \n",
      " [[22.53279993]\n",
      " [-0.83980839]\n",
      " [ 0.92612237]\n",
      " [-0.17541988]\n",
      " [ 0.72676226]\n",
      " [-1.82369448]\n",
      " [ 2.78447498]\n",
      " [-0.05650494]\n",
      " [-2.96695543]\n",
      " [ 1.80785186]\n",
      " [-1.1802415 ]\n",
      " [-1.99990382]\n",
      " [ 0.85595908]\n",
      " [-3.69524414]] \n",
      "\n",
      "최종 Loss:  [11.00713381]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_iters = 1500\n",
    "learning_rate = 0.01\n",
    "\n",
    "initial_cost = compute_cost(X, y, params)\n",
    "\n",
    "print('첫 Loss: ', initial_cost, '\\n')\n",
    "\n",
    "(J_history, optimal_params) = gradient_descent(X, y, params, learning_rate, n_iters)\n",
    "\n",
    "print('업데이트 된 파라미터: \\n', optimal_params, '\\n')\n",
    "\n",
    "print('최종 Loss: ', J_history[-1])\n",
    "\n",
    "plt.plot(range(len(J_history)), J_history, 'r')\n",
    "plt.title(\"비용함수 최적화 그래프\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a72573e",
   "metadata": {},
   "source": [
    "## Linear Regression 클래스화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "66c94e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression():\n",
    "    def __init__(self, X, y, alpha=0.03, n_iter=1500):\n",
    "        self.alpha = alpha\n",
    "        self.n_iter = n_iter\n",
    "        self.n_samples = len(y)\n",
    "        self.n_features = np.size(X, 1)\n",
    "        self.X = np.hstack((np.ones((self.n_samples, 1)), (X - np.mean(X, 0)) / np.std(X, 0)))\n",
    "        self.y = y[:, np.newaxis]\n",
    "        self.params = np.zeros((self.n_features + 1, 1))\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "\n",
    "    def fit(self):\n",
    "        for i in range(self.n_iter):\n",
    "            self.params = self.params - (self.alpha/self.n_samples) * self.X.T @ (self.X @ self.params - self.y)\n",
    "\n",
    "        self.intercept_ = self.params[0]\n",
    "        self.coef_ = self.params[1:]\n",
    "        return self\n",
    "    \n",
    "    # 결정계수(R-Squared)\n",
    "    # 회귀모델에서 독립변수가 종속변수를 얼만큼 설명해 주는지 가리키는 지표\n",
    "    # 설명력이라고 부르기도 한다.\n",
    "    # 결정계수가 높을수록 독립변수가 종속변수를 많이 설명한다는 뜻\n",
    "    # 이 계수는 독립변수의 수가 증가하면 상승한다.\n",
    "    # 실제로 종속변수를 잘 설명하지 못하는 변수가 추가되어도 증가하기 때문에 결정계수만 가지고 회귀 모델의 유용성을 판단하는 것은 다소 문제가 있다.\n",
    "        # -> 조정된 결정계수(Adjusted R-Squared)를 사용함\n",
    "    # 결정계수가 0.3(30%)이라고 하면 독립변수가 종속변수의 30% 정도를 설명한다는 뜻이다.\n",
    "    # 몇 퍼센트 이상이 유용하다고 말하기는 어렵지만 일반적으로 20%는 넘어야 한다고 보고, 외생변수를 최대한 통제하는 설계를 기반으로 조사된 데이터에서는 더 높은 수치가 나와야 한다.\n",
    "    \n",
    "    # 결정계수 계산 방법: R^2 = (SSE/SST) = 1 - (SSR/SST)\n",
    "    # SST(Total Sum of Squares) = \\sum_{i=1}^n(y_i = \\bar{y})^2\n",
    "        # SST는 관측값에서 관측값의 평균(혹은 추정치의 평균)을 뺀 결과의 총합\n",
    "    # SSE(Explained Sum of Squares) = \\sum_{i=1}^n(\\hat{y_i} - \\bar{y})^2\n",
    "        # SSE는 추정값에서 관측값의 평균(혹은 추정치의 평균)을 뺀 결과의 총합\n",
    "    # SSR(Residual Sum of Squares) = \\sum_{i=1}^n(y_i - \\hat{y_i})^2\n",
    "        # SSR은 관측값에서 추정값을 뺀 값, 즉 잔차의 총합\n",
    "    def score(self, X=None, y=None):\n",
    "        if X is None:\n",
    "            X = self.X\n",
    "        else:\n",
    "            n_samples = np.size(X, 0)\n",
    "            X = np.hstack((np.ones((n_samples, 1)), (X - np.mean(X, 0)) / np.std(X, 0)))\n",
    "\n",
    "        if y is None:\n",
    "            y = self.y\n",
    "        else:\n",
    "            y = y[:, np.newaxis]\n",
    "\n",
    "        y_pred = X @ self.params\n",
    "        score = 1 - (((y - y_pred)**2).sum() / ((y - y.mean())**2).sum())\n",
    "\n",
    "        return score\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples = np.size(X, 0)\n",
    "        y = np.hstack((np.ones((n_samples, 1)), (X-np.mean(X, 0)) / np.std(X, 0))) @ self.params\n",
    "        return y\n",
    "\n",
    "    def get_params(self):\n",
    "\n",
    "        return self.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b71b7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy:  0.7434822344695983\n",
      "test accuracy:  0.6753466437353814\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_boston()\n",
    "\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "regressor = LinearRegression(X_train, y_train).fit()\n",
    "train_accuracy = regressor.score()\n",
    "test_accuracy = regressor.score(X_test, y_test)\n",
    "\n",
    "print('train accuracy: ', train_accuracy)\n",
    "print('test accuracy: ', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f4a157",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
